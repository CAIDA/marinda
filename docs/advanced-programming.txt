Advanced Client Programming Guide
=================================
_ _

== Overview

This document covers the more advanced features and techniques
available with Marinda.  You can often improve the efficiency or
sophistication of your programs by taking advantage of these features,
but you can usually get by with just the material discussed in the
more introductory link:client-programming.html[Client Programming
Guide].

Material is grouped by topic wherever possible, but feel free to jump
around and read only the topics that interest you.  As with the Client
Programming Guide, you may want to work through the examples yourself
with your own Marinda setup.


== Obtaining node information

You can call methods on the Marinda connection object to obtain
information about the node on which your program is running.  This
information is useful for customizing the execution of your program.
For instance, you can bootstrap the execution of your program by using
the local node name to retrieve the appropriate configuration values
from tuples stored in the global tuple space.

The `node_id` method returns the numeric ID of the local node, and the
`node_name` method returns the user-configured name; for example,

[source, ruby]
----
>> $ts.node_id
=> 2
>> $ts.node_name
=> "nibbler"
----

NOTE: You can set the node name with the `node_name` configuration
      line in `local-config.yaml`.

Here's an example of using the node name to dynamically configure a
client (you could just as easily use the node ID rather than the node
name for this purpose):

[source, ruby]
----
$gc = $ts.global_commons
config = $gc.read ["CONFIG", $ts.node_name(), nil, nil]
method, rate_limit = config.values_at 2, 3
----

You would populate the global commons ahead of time with configuration
tuples like `["CONFIG", "nibbler", "icmp-echo", 10]`.

A more advanced node information is the _run ID_ obtained with the
`run_id` method.  This is an unsigned 48-bit integer randomly
generated by the local tuple-space server at start up (this value is
incorporated into the unique IDs generated by `gen_id`).  The run ID
is useful for detecting a restart of the local server between
invocations of your program.  For this scheme to work, you would need
to maintain the last known values (node ID, run ID) in the global
tuple space or on local disk.  If you detect a restart, you might
clear out stale system state.

== Streaming operations: `monitor_stream`, `consume_stream`

Because of the potential for high latency, special care is needed to
communicate efficiently over long distances on the global Internet, as
opposed to communicating on a local-area network.  In the
implementation of most tuple space operations, there must be a
low-level message exchange for each matching tuple.  Therefore,
communication latency is often the primary factor limiting execution
rate.  For example, if the round-trip time (RTT) between a local
server and the global server is 100ms, then the maximum achievable
message exchange rate is 10 messages/sec, and thus a client can only
execute 10 operations/sec in the global tuple space (however, this
100ms latency will not impact the execution rate of operations in the
local tuple space, which is one of the motivations for a two-level
hierarchy of tuple spaces).  The `write` operation is an exception,
since it does not require a response.  Clients can execute `write` at
a high rate, subject only to the execution speed of the endpoints and
the bandwidth (but not the latency) of the path between the endpoints.

Marinda provides the _streaming_ operations `monitor_stream` and
`consume_stream` for increased throughput in high-latency deployments.
These operations behave mostly like their non-streaming counterparts
`monitor` and `consume`, but there are some subtle differences because
streaming operations execute _asynchronously_.  In contrast, `monitor`
and `consume` execute _synchronously_; that is, these operations do
not retrieve the next matching tuple until the client is ready for the
next tuple.  In terms of implementation, a client signals it is ready
for the next tuple by executing the next iteration of the loop
implementing the `monitor` or `consume` operation (for example, the
loop `$ts.monitor(...) do ... end`).  With the streaming operations,
Marinda does not wait until the client is ready--all matching tuples
(both existing and future tuples) are streamed asynchronously from the
global server to a local server, where they are then buffered until
the client is ready for them.  As far as the client is concerned, it
is still iterating over the tuples one by one in an apparently
synchronous fashion, but the delay to retrieve each tuple is
dramatically reduced--from a 100-_millisecond_ RTT to a
100-_microsecond_ interprocess-communication delay.

For the client, using these streaming operations is no different than
using `monitor` or `consume`.  For example, here is code to print
out all matching tuples:

[source, ruby]
----
$ts.monitor_stream([]) do |tuple|
  p tuple
end
----

This might take 1 second to complete on 1000 matching tuples, whereas
`monitor` might take 100 seconds to complete with an RTT of 100ms.

Why not use streaming operations all the time, if they increase
throughput?  Streaming operations are not necessarily the best nor the
correct choice in all situations.  In particular, streaming operations
transfer all matching tuples asynchronously, and this may not be what
you want.  This is inefficient if you only want the first few matching
tuples, or you want to iterate over the tuples that match a loose
template in order to find the first tuple that meets a more stringent
condition on the values.  In either case, a potentially large number
of unneeded tuples may be transferred.  This not only wastes
bandwidth, but can also increase the memory usage of Marinda servers,
as these tuples will need to be buffered in memory by the local server
until the client can get to them (and they need to be buffered
specially by the global server in preparation for transmitting them).

Besides potential inefficiency, a streaming operation may not have the
right semantics for a given situation.  For example, a single
`consume_stream` will immediately remove all matching tuples from the
entire tuple space at once (as contrasted with `consume`, which only
removes one tuple per iteration).  If all the matching tuples were
intended for that one client, then this is the desired result.
However, there are useful coordination patterns in which tuples should
be distributed fairly amongst a set of threads/processes that are all
performing a `consume` or `take` on the same template; for example,
the Bag-of-Tasks and the Master-Slave patterns are common ways of
exploiting embarrassingly parallel tasks.  Because `consume_stream`
may lead to an unfair distribution of tuples, it is unsuitable for
implementing these patterns.
